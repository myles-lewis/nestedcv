% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nestedcv.R
\name{nestcv.glmnet}
\alias{nestcv.glmnet}
\title{Nested cross-validation with glmnet}
\usage{
nestcv.glmnet(
  y,
  x,
  family = c("gaussian", "binomial", "poisson", "multinomial", "cox", "mgaussian"),
  filterFUN = NULL,
  filter_options = NULL,
  outer_method = c("cv", "LOOCV"),
  n_outer_folds = 10,
  n_inner_folds = 10,
  alphaSet = seq(0, 1, 0.1),
  min_1se = 0,
  keep = TRUE,
  penalty.factor = rep(1, ncol(x)),
  cv.cores = 1,
  ...
)
}
\arguments{
\item{y}{Response vector}

\item{x}{Matrix of predictors}

\item{family}{Either a character string representing one of the built-in
families, or else a \code{glm()} family object. Passed to \link{cv.glmnet} and
\link{glmnet}}

\item{filterFUN}{Filter function, e.g. \link{ttest_filter} or \link{relieff_filter}.
Any function can be provided and is passed \code{y} and \code{x}. Must return a
character vector with names of filtered predictors.}

\item{filter_options}{List of additional arguments passed to the filter
function specified by \code{filterFUN}.}

\item{outer_method}{String of either \code{"cv"} or \code{"LOOCV"} specifying whether
to do k-fold CV or leave one out CV (LOOCV) for the outer folds}

\item{n_outer_folds}{Number of outer CV folds}

\item{n_inner_folds}{Number of inner CV folds}

\item{alphaSet}{Vector of alphas to be tuned}

\item{min_1se}{Value from 0 to 1 specifying choice of optimal lambda from
0=lambda.min to 1=lambda.1se}

\item{keep}{Logical indicating whether inner CV predictions are retained for
calculating left-out inner CV fold accuracy etc. See argument \code{keep} in
\link{cv.glmnet}.}

\item{penalty.factor}{Separate penalty factors can be applied to each
coefficient. Can be 0 for some variables, which implies no shrinkage, and
that variable is always included in the model. Default is 1 for all
variables. See \link{glmnet}}

\item{cv.cores}{Number of cores for parallel processing. Note this currently
uses \link[parallel:mclapply]{parallel::mclapply}.}

\item{...}{Optional arguments passed to \link{cv.glmnet}}
}
\value{
An object with S3 class "nestcv.glmnet"
\item{call}{the matched call}
\item{output}{Predictions on the left-out outer folds}
\item{outer_result}{List object of results from each outer fold containing
predictions on left-out outer folds, best lambda, best alpha, fitted glmnet
coefficients, list object of inner fitted cv.glmnet and number of filtered
predictors at each fold.}
\item{outer_method}{the \code{outer_method} argument}
\item{n_inner_folds}{number of inner folds}
\item{outer_folds}{List of
indices of outer training folds}
\item{dimx}{dimensions of \code{x}}
\item{final_param}{Final mean best lambda
and alpha from each fold}
\item{final_fit}{Final fitted glmnet model}
\item{roc}{ROC AUC for binary classification where available.}
\item{summary}{Overall performance summary. Accuracy and balanced accuracy
for classification. ROC AUC for binary classification. RMSE for
regression.}
}
\description{
This function enables nested cross-validation (CV) with glmnet including
tuning of elastic net alpha parameter. The function also allows the option of
embedded filtering of predictors for feature selection nested within the
outer loop of CV. Predictions on the outer test folds are brought back
together and error estimation/ accuracy determined. The default is 10x10
nested CV.
}
\examples{

## Example binary classification problem with P >> n
x <- matrix(rnorm(150 * 2e+04), 150, 2e+04)  # predictors
y <- factor(rbinom(150, 1, 0.5))  # binary response

## Partition data into 2/3 training set, 1/3 test set
trainSet <- caret::createDataPartition(y, p = 0.66, list = FALSE)

## t-test filter using whole dataset
filt <- ttest_filter(y, x, nfilter = 100)
filx <- x[, filt]

## Train glmnet on training set only using filtered predictor matrix
library(glmnet)
fit <- cv.glmnet(filx[trainSet, ], y[trainSet], family = "binomial")
plot(fit)

## Predict response on test set
predy <- predict(fit, newx = filx[-trainSet, ], s = "lambda.min", type = "class")
predy <- as.vector(predy)
predyp <- predict(fit, newx = filx[-trainSet, ], s = "lambda.min", type = "response")
predyp <- as.vector(predyp)
output <- data.frame(testy = y[-trainSet], predy = predy, predyp = predyp)

## Results on test set
## shows bias since univariate filtering was applied to whole dataset
predSummary(output)

## Nested CV
fit2 <- nestcv.glmnet(y, x, family = "binomial", alphaSet = 1,
                      filterFUN = ttest_filter,
                      filter_options = list(nfilter = 100))
summary(fit2)

}
\author{
Myles Lewis
}
